{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "XxM0K8I6PjOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1.  What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "Answer ->\n",
        "\n",
        "**Boosting :**\n",
        "Boosting in Machine Learning is an ensemble learning technique that combines multiple weak learners (usually simple models like shallow decision trees) to create a strong learner with improved predictive performance.\n",
        "\n",
        "**How Boosting Improves Weak Learners :**\n",
        "\n",
        "Boosting improves weak learners by:\n",
        "\n",
        "- Focusing on mistakes: Later models learn where earlier ones went wrong.\n",
        "\n",
        "- Weighted combination: It assigns higher importance to more accurate models.\n",
        "\n",
        "- Sequential learning: Each learner contributes a small correction, and together they form a highly accurate model.\n",
        "\n",
        "As a result, boosting can convert multiple weak models into one strong model with much better accuracy."
      ],
      "metadata": {
        "id": "jvcYfFylPjLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "**AdaBoost :**\n",
        "\n",
        "- It focuses on re-weighting data points — increases weights of misclassified samples\n",
        "- It uses sample weights to make the next model pay more attention to difficult examples\n",
        "- It is less flexible\n",
        "- Common use in Classification\n",
        "\n",
        "**Gradient Boosting :**\n",
        "\n",
        "- It focuses on minimizing residual errors — fits new models to the gradients (residuals) of a loss function\n",
        "- It uses gradients (direction of maximum error reduction) to correct previous model’s mistakes\n",
        "- It is more flexible\n",
        "- Common use in Classification and Regression"
      ],
      "metadata": {
        "id": "UcuyqWe7PjIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3. How does regularization help in XGBoost?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "Regularization in XGBoost helps control model complexity and prevent overfitting by adding penalty terms to the objective function. It includes L1 (alpha) and L2 (lambda) regularization on leaf weights, which shrink or eliminate less important features, and gamma (γ), which penalizes the addition of too many leaves in a tree. These penalties discourage overly deep or complex trees and ensure smoother, more generalizable predictions. By balancing model accuracy and simplicity, regularization allows XGBoost to maintain strong performance on training data while improving its ability to generalize well to unseen or noisy data in real-world applications."
      ],
      "metadata": {
        "id": "zg_h_78hPjF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4. Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "**CatBoost** is efficient for handling **categorical data** because it has a **built-in mechanism** to process categorical features directly, without requiring manual preprocessing like one-hot or label encoding. It uses a technique called **ordered target encoding**, which converts categorical values into numerical representations based on target statistics while preventing **target leakage**. This approach allows CatBoost to learn meaningful relationships from categorical features without overfitting. Additionally, it efficiently handles **high-cardinality** features (those with many unique categories) and reduces both **training time** and **memory usage**, making it faster and more accurate than other gradient boosting methods for categorical data.\n",
        "\n"
      ],
      "metadata": {
        "id": "IHWFOxnvPjCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5.  What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "Boosting techniques are preferred over bagging methods in real-world applications where high accuracy, complex decision boundaries, and minimizing bias are crucial. Unlike bagging (which reduces variance), boosting focuses on reducing bias by combining weak learners sequentially to correct previous errors.\n",
        "\n",
        "Here are some key applications :     \n",
        "- Credit Scoring and Loan Default Prediction (Finance)\n",
        "- Fraud Detection (Banking & E-commerce)\n",
        "- Customer Churn Prediction (Telecom & Marketing)\n",
        "- Medical Diagnosis and Disease Prediction (Healthcare)\n",
        "- Click-Through Rate (CTR) Prediction (Online Advertising)\n",
        "- Stock Market Forecasting and Risk Management"
      ],
      "metadata": {
        "id": "H3YqPsgbPjAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6. Datasets:\n",
        "\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "Question : Write a Python program to:\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n"
      ],
      "metadata": {
        "id": "E7-0qOWTPi8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGwiuqF6Wbnt",
        "outputId": "2a9b0118-f5e6-4385-953a-33d46cdd565a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7. Write a Python program to:\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "CfLP1xBHPi5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLwo02FeWpIM",
        "outputId": "72e2c5ba-cfd7-4a18-ce53-d5e70d99ea3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. Write a Python program to:\n",
        "\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "SOnSTr-4Pi2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Predict on test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMz2KyHEXN5N",
        "outputId": "5f9b1f18-89b8-4e48-f268-9bf8f05a2f10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Set Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:49:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. Write a Python program to:\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "SSH77dodPizY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, verbose=0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H6zpm8MFYK0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "Answer ->\n",
        "\n",
        "Data Cleaning: Handle missing values.\n",
        "\n",
        "Feature Processing: Encode categorical variables; scale if needed.\n",
        "\n",
        "Imbalance Handling: Class weighting or resampling.\n",
        "\n",
        "Model Selection: CatBoost/XGBoost for robust boosting.\n",
        "\n",
        "Hyperparameter Tuning: CV-based search for learning rate, depth, regularization.\n",
        "\n",
        "Model Evaluation: ROC-AUC, Precision-Recall, F1-score.\n",
        "\n",
        "Business Impact: Reduced default, better loan allocation, and regulatory insight."
      ],
      "metadata": {
        "id": "vkfcFtpzYNu-"
      }
    }
  ]
}